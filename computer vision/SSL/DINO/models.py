from turtle import forward
from typing import Optional
from sympy import Mul
import torch
from torch import nn
import math
import warnings
from .BackBoneType import BackBoneType
from .utils import MultiCropWrapper

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class DINOHead(nn.Module):
    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x

class VitSmallPatch16(nn.Module):
    def __init__(self, pretrained=True) -> None:
        super(VitSmallPatch16).__init__()
        self.backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=pretrained)
        self.fc = nn.Identity()
        self.head = self.backbone.head
        self.backbone.head = nn.Identity()
        self.patch_size = 16

    @property
    def embed_dim(self)->int:
        return self.backbone.embed_dim

    def forward(self, x):
        x = self.backbone(x)
        x = self.head(x)
        return x

    def get_attention(self,x:torch.Tensor, threshold:Optional[float]=None):
        w_featmap = x.size()[-2] // self.patch_size
        h_featmap = x.size()[-1] // self.patch_size
        
        with torch.no_grad():
            attentions:torch.Tensor = self.backbone.get_last_selfattention(x)
        nh = attentions.size()[1] # number of head

        # we keep only the output patch attention
        attentions = attentions[0, :, 0, 1:].reshape(nh, -1)

        if threshold is not None:
            # we keep only a certain percentage of the mass
            val, idx = torch.sort(attentions)
            val /= torch.sum(val, dim=1, keepdim=True)
            cumval = torch.cumsum(val, dim=1)
            th_attn = cumval > (1 - threshold)
            idx2 = torch.argsort(idx)
            for head in range(nh):
                th_attn[head] = th_attn[head][idx2[head]]
            th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()
            # interpolate
            attentions = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=self.patch_size, mode="nearest")[0].cpu().numpy()
        else:
            attentions = attentions.reshape(nh, w_featmap, h_featmap)
            attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=self.patch_size, mode="nearest")[0].cpu().numpy()
        return attentions        


def get_student_teacher(backbone_type:BackBoneType, out_dim:int, pretrained:bool=True, use_bn:bool=False,
                                                    norm_last_layer:bool=True, nlayers:int=3, hidden_dim:int=2048, bottleneck_dim:int=256):
    if backbone_type == BackBoneType.VITSMALLBATCH16:
        student = VitSmallPatch16(pretrained=pretrained)
        teacher = VitSmallPatch16(pretrained=False)
        embed_dim = student.embed_dim
    student = MultiCropWrapper(
        student,
        DINOHead(
            in_dim=embed_dim,
            out_dim=out_dim,
            use_bn=use_bn,
            norm_last_layer=norm_last_layer,
            nlayers=nlayers,
            hidden_dim=hidden_dim,
            bottleneck_dim=bottleneck_dim,
            ))
    teacher = MultiCropWrapper(
        teacher, 
        DINOHead(in_dim=embed_dim, out_dim=out_dim, use_bn=use_bn),
    )
    teacher.load_state_dict(student.state_dict())
    for p in teacher.parameters():
        p.requires_grad = False
    print(f"Student and Teacher are built")
    
    return student, teacher
